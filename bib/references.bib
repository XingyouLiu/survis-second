@article{Barke2023GroundedCopilot,
author = {Barke, Shraddha and James, Michael B. and Polikarpova, Nadia},
title = {Grounded Copilot: How Programmers Interact with Code-Generating Models},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586030},
doi = {10.1145/3586030},
abstract = {Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants—with a range of prior experience using the assistant—as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {78},
numpages = {27},
keywords = {type:Interaction mechanics and strategies of coding assistance, AI Assistants, Grounded Theory, Program Synthesis},
series = {OOPSLA '23}
}

@inproceedings{Mozannar2024ReadingLines,
author = {Mozannar, Hussein and Bansal, Gagan and Fourney, Adam and Horvitz, Eric},
title = {Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641936},
doi = {10.1145/3613904.3641936},
abstract = {Code-recommendation systems, such as Copilot and CodeWhisperer, have the potential to improve programmer productivity by suggesting and auto-completing code. However, to fully realize their potential, we must understand how programmers interact with these systems and identify ways to improve that interaction. To seek insights about human-AI collaboration with code recommendations systems, we studied GitHub Copilot, a code-recommendation system used by millions of programmers daily. We developed CUPS, a taxonomy of common programmer activities when interacting with Copilot. Our study of 21 programmers, who completed coding tasks and retrospectively labeled their sessions with CUPS, showed that CUPS can help us understand how programmers interact with code-recommendation systems, revealing inefficiencies and time costs. Our insights reveal how programmers interact with Copilot and motivate new interface designs and metrics.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {142},
numpages = {16},
keywords = {type:Interaction mechanics and strategies of coding assistance, AI-assisted Programming, Copilot, User State Model},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{Mendes2024BicycleMotor,
author = {Mendes, Wendy and Souza, Samara and De Souza, Cleidson},
title = {"You're on a bicycle with a little motor": Benefits and Challenges of Using AI Code Assistants},
year = {2024},
isbn = {9798400705335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641822.3641882},
doi = {10.1145/3641822.3641882},
abstract = {AI code assistants, such as Tabnine, GitHub CoPilot, and ChatGPT, employ Large Language Models (LLMs) trained on extensive source code and other documents. They receive prompts and generate code suggestions aimed to facilitate programming tasks. Previous research in this field has explored the correctness, complexity, quality, and security of the code suggestions. Software developers' experiences have been studied in the context of controlled experiments. Based on 14 interviews with software developers, this paper describes the developers' daily and continuous experiences with AI code assistants, presenting benefits and challenges grounded in actual development work, along with strategies to address these challenges.},
booktitle = {Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering},
pages = {144–152},
numpages = {9},
keywords = {type:Adoption and usability of coding assistance, AI code assistants, developer experiences, code generation},
location = {Lisbon, Portugal},
series = {CHASE '24}
}

@inproceedings{Coutinho2024GenerativeAIRole,
author = {Coutinho, Mariana and Marques, Lorena and Santos, Anderson and Dahia, Marcio and Fran\c{c}a, Cesar and de Souza Santos, Ronnie},
title = {The Role of Generative AI in Software Development Productivity: A Pilot Case Study},
year = {2024},
isbn = {9798400706851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664646.3664773},
doi = {10.1145/3664646.3664773},
abstract = {With software development increasingly reliant on innovative technologies, there is a growing interest in exploring the potential of generative AI tools to streamline processes and enhance productivity. In this scenario, this paper investigates the integration of generative AI tools within software development, focusing on understanding their uses, benefits, and challenges to software professionals, in particular, looking at aspects of productivity. Through a pilot case study involving software practitioners working in different roles, we gathered valuable experiences on the integration of generative AI tools into their daily work routines. Our findings reveal a generally positive perception of these tools in individual productivity while also highlighting the need to address identified limitations. Overall, our research sets the stage for further exploration into the evolving landscape of software development practices with the integration of generative AI tools.},
booktitle = {Proceedings of the 1st ACM International Conference on AI-Powered Software},
pages = {131–138},
numpages = {8},
keywords = {type:Impact and perceived value of coding assistance, LLMs, generative AI, productivity, software engineering},
location = {Porto de Galinhas, Brazil},
series = {AIware 2024}
}

@inproceedings{Liang2024LargeScaleSurvey,
author = {Liang, Jenny T. and Yang, Chenyang and Myers, Brad A.},
title = {A Large-Scale Survey on the Usability of AI Programming Assistants: Successes and Challenges},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608128},
doi = {10.1145/3597503.3608128},
abstract = {The software engineering community recently has witnessed widespread deployment of AI programming assistants, such as GitHub Copilot. However, in practice, developers do not accept AI programming assistants' initial suggestions at a high frequency. This leaves a number of open questions related to the usability of these tools. To understand developers' practices while using these tools and the important usability challenges they face, we administered a survey to a large population of developers and received responses from a diverse set of 410 developers. Through a mix of qualitative and quantitative analyses, we found that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions. We also found the most important reasons why developers do not use these tools are because these tools do not output code that addresses certain functional or non-functional requirements and because developers have trouble controlling the tool to generate the desired output. Our findings have implications for both creators and users of AI programming assistants, such as designing minimal cognitive effort interactions with these tools to reduce distractions for users while they are programming.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {52},
numpages = {13},
keywords = {type:Adoption and usability of coding assistance, AI programming assistants, usability study},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{Weisz2025ExaminingUseImpact,
author = {Weisz, Justin D. and Kumar, Shraddha Vijay and Muller, Michael and Browne, Karen-Ellen and Goldberg, Arielle and Heintze, Katrin Ellice and Bajpai, Shagun},
title = {Examining the Use and Impact of an AI Code Assistant on Developer Productivity and Experience in the Enterprise},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3706670},
doi = {10.1145/3706599.3706670},
abstract = {AI assistants are being created to help software engineers conduct a variety of coding-related tasks, such as writing, documenting, and testing code. We describe the use of the watsonx Code Assistant (WCA), an LLM-powered coding assistant deployed internally within IBM. Through surveys of two user cohorts (N=669) and unmoderated usability testing (N=15), we examined developers’ experiences with WCA and its impact on their productivity. We learned about their motivations for using (or not using) WCA, we examined their expectations of its speed and quality, and we identified new considerations regarding ownership of and responsibility for generated code. Our case study characterizes the impact of an LLM-powered assistant on developers’ perceptions of productivity and it shows that although such tools do often provide net productivity increases, these benefits may not always be experienced by all users.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {673},
numpages = {13},
keywords = {type:Impact and perceived value of coding assistance, Generative AI, LLM, software engineering, productivity, code assistant},
location = {
},
series = {CHI EA '25}
}

@article{Sergeyuk2025UsingAIAssistants,
title = {Using AI-based coding assistants in practice: State of affairs, perceptions, and ways forward},
journal = {Information and Software Technology},
volume = {178},
pages = {107610},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107610},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924002155},
author = {Agnia Sergeyuk and Yaroslav Golubev and Timofey Bryksin and Iftekhar Ahmed},
keywords = {type:Adoption and usability of coding assistance, LLMs, AI assistants, Software development lifecycle},
series = {Information and Software Technology},
abstract = {Context:
The last several years saw the emergence of AI assistants for code — multi-purpose AI-based helpers in software engineering. As they become omnipresent in all aspects of software development, it becomes critical to understand their usage patterns.
Objective:
We aim to better understand how specifically developers are using AI assistants, why they are not using them in certain parts of their development workflow, and what needs to be improved in the future.
Methods:
In this work, we carried out a large-scale survey aimed at how AI assistants are used, focusing on specific software development activities and stages. We collected opinions of 481 programmers on five broad activities: (a) implementing new features, (b) writing tests, (c) bug triaging, (d) refactoring, and (e) writing natural-language artifacts, as well as their individual stages.
Results:
Our results provide a novel comparison of different stages where AI assistants are used that is both comprehensive and detailed. It highlights specific activities that developers find less enjoyable and want to delegate to an AI assistant, e.g., writing tests and natural-language artifacts. We also determine more granular stages where AI assistants are used, such as generating tests and generating docstrings, as well as less studied parts of the workflow, such as generating test data. Among the reasons for not using assistants, there are general aspects like trust and company policies, as well as more concrete issues like the lack of project-size context, which can be the focus of the future research.
Conclusion:
The provided analysis highlights stages of software development that developers want to delegate and that are already popular for using AI assistants, which can be a good focus for features aimed to help developers right now. The main reasons for not using AI assistants can serve as a guideline for future work.}
}

@inproceedings{Nguyen2024HowBeginnersMisread,
author = {Nguyen, Sydney and Babe, Hannah McLean and Zi, Yangtian and Guha, Arjun and Anderson, Carolyn Jane and Feldman, Molly Q},
title = {How Beginning Programmers and Code LLMs (Mis)read Each Other},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642706},
doi = {10.1145/3613904.3642706},
keywords={type:Interaction mechanics and strategies of coding assistance},
abstract = {Generative AI models, specifically large language models (LLMs), have made strides towards the long-standing goal of text-to-code generation. This progress has invited numerous studies of user interaction. However, less is known about the struggles and strategies of non-experts, for whom each step of the text-to-code problem presents challenges: describing their intent in natural language, evaluating the correctness of generated code, and editing prompts when the generated code is incorrect. This paper presents a large-scale controlled study of how 120 beginning coders across three academic institutions approach writing and editing prompts. A novel experimental design allows us to target specific steps in the text-to-code process and reveals that beginners struggle with writing and editing prompts, even for problems at their skill level and when correctness is automatically determined. Our mixed-methods evaluation provides insight into student processes and perceptions with key implications for non-expert Code LLM use within and outside of education.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {651},
numpages = {26},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@INPROCEEDINGS{Davila2024IndustryCaseStudy,
  author={Davila, Nicole and Wiese, Igor and Steinmacher, Igor and Da Silva, Lucas Lucio and Kawamoto, André and Peres Favaro, Gilson José and Nunes, Ingrid},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)}, 
  title={An Industry Case Study on Adoption of AI-based Programming Assistants}, 
  year={2024},
  volume={},
  number={},
  pages={92-102},
  keywords={type:Adoption and usability of coding assistance, Industries;Surveys;Computer languages;Programming;Syntactics;Chatbots;Software;Artificial Intelligence;Generative AI;ChatGPT;Industry Case Study;Software Development},
  doi={10.1145/3639477.3643648},
  series = {ICSE-SEIP '24}
  }


@inproceedings{Kazemitabaar2024NovicesLLM,
author = {Kazemitabaar, Majeed and Hou, Xinying and Henley, Austin and Ericson, Barbara Jane and Weintrop, David and Grossman, Tovi},
title = {How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631806},
doi = {10.1145/3631802.3631806},
abstract = {As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners’ utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners’ use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {3},
numpages = {12},
keywords = {type:Interaction mechanics and strategies of coding assistance, ChatGPT, Copilot, Introductory Programming, Large Language Models, OpenAI Codex, Self-paced Learning, Self-regulation},
location = {Koli, Finland},
series = {Koli Calling '23}
}